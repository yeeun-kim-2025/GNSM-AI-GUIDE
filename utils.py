# utils.py â€” Grounded LLM + ì˜¤íƒ€ ìë™ ë³´ì • + ì£¼ì œ ë²„íŠ¼ ë²„ì „
# ì‹¤ì‹œê°„ requests + HTML â†’ FACTS + ì‚¬ì´íŠ¸ê²€ìƒ‰ + LLM(FACTSë§Œ ìš”ì•½/ë§íˆ¬)

import os
import traceback
import logging
import re
import difflib

import requests
import streamlit as st
from bs4 import BeautifulSoup

from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage

MUSEUM_BASE_URL = "https://www.sciencecenter.go.kr"


# ---------------------------------------
# 0. ë¡œê±°
# ---------------------------------------
def init_logger():
    logger = logging.getLogger()
    if logger.handlers:
        return
    logger.setLevel(logging.INFO)
    handler = logging.StreamHandler()
    handler.setFormatter(logging.Formatter("%(asctime)s [%(levelname)s] %(message)s"))
    logger.addHandler(handler)


init_logger()


# ---------------------------------------
# 1. ê³µí†µ ìœ í‹¸: ì‹œê°„ í‘œí˜„ ë³´ì •, ì œëª© ì¶”ì¶œ
# ---------------------------------------
_TIME_RANGE_PATTERN = re.compile(r"(\d{1,2}:\d{2})\s*(~|âˆ¼|-|â€“|â€”)?\s*(\d{1,2}:\d{2})")


def _normalize_time_ranges(text: str) -> str:
    """
    HTMLì—ì„œ ì‹œê°„ êµ¬ê°„ì´ '10:0010:40' ì²˜ëŸ¼ ë¶™ì–´ ìˆëŠ” ê²½ìš°
    '10:00~10:40' í˜•íƒœë¡œ ìë™ ë³´ì •í•œë‹¤.
    ì´ë¯¸ ~, âˆ¼, -, â€“, â€” ë“±ì´ ë“¤ì–´ìˆëŠ” ê²½ìš°ëŠ” ê·¸ëŒ€ë¡œ ë‘”ë‹¤.
    """

    def repl(m: re.Match):
        start, sep, end = m.group(1), m.group(2), m.group(3)
        # ì´ë¯¸ êµ¬ë¶„ìê°€ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        if sep and sep.strip():
            return f"{start}{sep}{end}"
        # êµ¬ë¶„ìê°€ ì—†ìœ¼ë©´ ~ë¡œ ì±„ì›Œì¤€ë‹¤
        return f"{start}~{end}"

    return _TIME_RANGE_PATTERN.sub(repl, text)


def _extract_page_title(html: str) -> str:
    """
    í˜ì´ì§€ì˜ ì œëª© í›„ë³´ë¥¼ ì¶”ì¶œí•œë‹¤.
    - ê³µì§€ ë³¸ë¬¸ ìƒë‹¨ h1/h2/h3
    - ì—†ìœ¼ë©´ <title> íƒœê·¸
    """
    soup = BeautifulSoup(html, "lxml")
    for tag in ["h1", "h2", "h3"]:
        el = soup.find(tag)
        if el and el.get_text(strip=True):
            return " ".join(el.stripped_strings)
    if soup.title and soup.title.string:
        return soup.title.string.strip()
    return ""


# ---------------------------------------
# 1-1. LLM ë‹µë³€ ë§ˆí¬ë‹¤ìš´ ì •ë¦¬ (ì·¨ì†Œì„  ì œê±°)
# ---------------------------------------
def _cleanup_answer_markdown(answer: str) -> str:
    """
    LLM ë‹µë³€ ì•ˆì— ë“¤ì–´ê°„ ~~ì·¨ì†Œì„ ~~ ë§ˆí¬ë‹¤ìš´ì„ ì œê±°í•œë‹¤.
    - 20:00~~21:30 â†’ 20:00~21:30
    - ~~ë¬¸ì¥~~ â†’ ë¬¸ì¥
    """
    # ì‹œê°„ êµ¬ê°„ì— ì˜ëª» ë“¤ì–´ê°„ ~~ ë¥¼ í•œ ê°œì˜ ~ ë¡œ í†µì¼
    answer = re.sub(
        r"(\d{1,2}:\d{2})\s*~~\s*(\d{1,2}:\d{2})",
        r"\1~\2",
        answer,
    )
    # ì¼ë°˜ì ì¸ ~~í…ìŠ¤íŠ¸~~ ì·¨ì†Œì„  ì œê±°
    answer = re.sub(r"~~([^~]+?)~~", r"\1", answer)
    return answer


# ---------------------------------------
# 2. HTML â†’ FACTS (í…ìŠ¤íŠ¸/í‘œ/ì´ë¯¸ì§€)
# ---------------------------------------
def _find_html_with_table(obj):
    """JSON / dict / list ì•ˆì—ì„œ <table>ì´ ë“¤ì–´ìˆëŠ” HTML ë¬¸ìì—´ì„ ì¬ê·€ì ìœ¼ë¡œ ì°¾ê¸°."""
    if isinstance(obj, str) and "<table" in obj.lower():
        return obj
    if isinstance(obj, dict):
        for v in obj.values():
            found = _find_html_with_table(v)
            if found:
                return found
    if isinstance(obj, list):
        for v in obj:
            found = _find_html_with_table(v)
            if found:
                return found
    return None


def _extract_tables_from_html(html: str, max_tables: int = 10) -> str:
    """HTML ì•ˆì˜ <table>ë“¤ì„ ë§ˆí¬ë‹¤ìš´ í‘œë¡œ ë³€í™˜ (ëª¨ë“  í–‰/ì—´ í¬í•¨)."""
    soup = BeautifulSoup(html, "lxml")
    tables = soup.find_all("table")
    if not tables:
        return ""

    blocks = []
    for table in tables[:max_tables]:
        rows = []
        for tr in table.find_all("tr"):
            row = []
            for td in tr.find_all(["th", "td"]):
                links = td.find_all("a")
                if links:
                    parts = []
                    for a in links:
                        text = " ".join(a.stripped_strings) or "ìì„¸íˆ ë³´ê¸°"
                        href = (a.get("href") or "").strip()
                        if href.startswith("/"):
                            href = MUSEUM_BASE_URL + href
                        if href:
                            parts.append(f"[{text}]({href})")
                        else:
                            parts.append(text)
                    cell_text = " ".join(parts)
                else:
                    cell_text = " ".join(td.stripped_strings)

                # ì‹œê°„ êµ¬ê°„ í‘œê¸° ë³´ì • (10:0010:40 â†’ 10:00~10:40)
                cell_text = _normalize_time_ranges(cell_text)

                row.append(cell_text)
            if row:
                rows.append(row)

        if not rows:
            continue

        max_cols = max(len(r) for r in rows)
        rows = [r + [""] * (max_cols - len(r)) for r in rows]

        header = rows[0]
        body = rows[1:]

        lines = [
            "| " + " | ".join(header) + " |",
            "| " + " | ".join(["---"] * max_cols) + " |",
        ]
        for r in body:
            lines.append("| " + " | ".join(r) + " |")
        blocks.append("\n".join(lines))

    return "\n\n".join(blocks)


def _extract_tables_and_images_for_display(html: str):
    """
    HTML ì „ì²´ì—ì„œ í‘œì™€ ì´ë¯¸ì§€ URLì„ ë³„ë„ë¡œ ì¶”ì¶œí•œë‹¤.
    (ì§€ê¸ˆì€ 'ì¡´ì¬ ì—¬ë¶€'ë§Œ í™•ì¸í•´ì„œ ì•ˆë‚´ ë¬¸êµ¬ë¥¼ ë„ìš°ëŠ” ë° ì‚¬ìš©)
    """
    soup = BeautifulSoup(html, "lxml")
    # ìŠ¤í¬ë¦½íŠ¸/ìŠ¤íƒ€ì¼ ë“± ì œê±°
    for t in soup(["script", "style", "noscript", "header", "footer", "nav"]):
        t.decompose()

    tables_md = _extract_tables_from_html(str(soup))

    image_urls = []
    for img in soup.find_all("img"):
        src = (img.get("src") or "").strip()
        if not src:
            continue
        if src.startswith("/"):
            src = MUSEUM_BASE_URL + src
        if src not in image_urls:
            image_urls.append(src)

    return tables_md, image_urls


def _html_to_facts(html: str) -> str:
    """
    HTML ì „ì²´ë¥¼ FACTSë¡œ ë³€í™˜.
    - <h1~h4>, <p>, <li>ë¥¼ í•œ ì¤„ì”© ì •ë¦¬í•´ì„œ êµ¬ì¡°ë¥¼ ìµœëŒ€í•œ ì‚´ë¦¼
    - í‘œëŠ” ë§ˆí¬ë‹¤ìš´ í‘œë¡œ ì „ì²´ ì¶”ì¶œ
    - ì´ë¯¸ì§€ srcë„ ëª¨ë‘ FACTSì— í¬í•¨
    - ì¤‘ê°„ì—ì„œ ìë¥´ì§€ ì•Šê³  ì „ì²´ ì‚¬ìš©
    """
    soup = BeautifulSoup(html, "lxml")

    # ìŠ¤í¬ë¦½íŠ¸/ìŠ¤íƒ€ì¼ ë“± ì œê±°
    for t in soup(["script", "style", "noscript", "header", "footer", "nav"]):
        t.decompose()

    # í…ìŠ¤íŠ¸: ë¸”ë¡ ìš”ì†Œë³„ë¡œ í•œ ì¤„ì”©
    lines = []
    for elem in soup.find_all(["h1", "h2", "h3", "h4", "p", "li"]):
        text = " ".join(elem.stripped_strings)
        if not text:
            continue

        text = _normalize_time_ranges(text)

        if elem.name == "li":
            lines.append(f"- {text}")
        elif elem.name in ["h1", "h2", "h3", "h4"]:
            level = int(elem.name[1])
            level = min(level, 4)
            prefix = "#" * level
            lines.append(f"{prefix} {text}")
        else:
            lines.append(text)

    text_block = "\n".join(lines)

    # í‘œ
    tables_md = _extract_tables_from_html(str(soup))

    # ì´ë¯¸ì§€ (FACTSìš© URL ë‚˜ì—´)
    image_urls = []
    for img in soup.find_all("img"):
        src = (img.get("src") or "").strip()
        if not src:
            continue
        if src.startswith("/"):
            src = MUSEUM_BASE_URL + src
        if src not in image_urls:
            image_urls.append(src)

    parts = []
    if text_block:
        parts.append("### í…ìŠ¤íŠ¸\n" + text_block)
    if tables_md:
        parts.append("### í‘œ\n" + tables_md)
    if image_urls:
        parts.append("### ì´ë¯¸ì§€ URL\n" + "\n".join(image_urls))

    if not parts:
        return "ì´ í˜ì´ì§€ì—ì„œ í…ìŠ¤íŠ¸ë‚˜ í‘œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

    return "\n\n".join(parts)


# ---------------------------------------
# 3. ì‹¤ì‹œê°„ í˜ì´ì§€ fetch (showBoard API + ì¼ë°˜ GET)
# ---------------------------------------
def _fetch_page(url: str) -> dict:
    """
    urlì—ì„œ ë‚´ìš©ì„ ê°€ì ¸ì™€ì„œ
    {
      "source": url,
      "title": í˜ì´ì§€ ì œëª©(ìˆìœ¼ë©´),
      "facts": FACTS í…ìŠ¤íŠ¸,
      "has_rich": í‘œë‚˜ ì´ë¯¸ì§€ê°€ í•˜ë‚˜ë¼ë„ ìˆìœ¼ë©´ True,
      í˜¹ì€ "error": "..."
    } í˜•íƒœë¡œ ë°˜í™˜.
    """
    logging.info(f"[LIVE] URL ê°€ì ¸ì˜¤ê¸°: {url}")
    try:
        # ê³µì§€ ìƒì„¸: ë‚´ë¶€ showBoard JSON ë¨¼ì € ì‹œë„
        m = re.search(r"/scipia/introduce/notice/(\d+)", url)
        if m:
            board_id = m.group(1)
            api_url = f"{MUSEUM_BASE_URL}/scipia/boards/showBoard/{board_id}"
            headers = {
                "User-Agent": "GNSM-AI-Docent/1.0",
                "Accept": "application/json, text/javascript, */*; q=0.01",
                "X-Requested-With": "XMLHttpRequest",
            }
            resp = requests.post(api_url, headers=headers, json={}, timeout=10)
            resp.raise_for_status()
            data = resp.json()

            html = _find_html_with_table(data)
            if not html and isinstance(data, dict) and isinstance(data.get("content"), str):
                html = data["content"]

            title = ""
            if isinstance(data, dict):
                for k in ["title", "subject", "boardTitle"]:
                    if k in data and isinstance(data[k], str):
                        title = data[k].strip()
                        break

            if html:
                facts = _html_to_facts(html)
                tables_md, image_urls = _extract_tables_and_images_for_display(html)
                if not title:
                    title = _extract_page_title(html)
                has_rich = bool(tables_md or image_urls)
                return {
                    "source": url,
                    "title": title,
                    "facts": facts,
                    "has_rich": has_rich,
                }

        # ê·¸ ì™¸ëŠ” ì¼ë°˜ GET
        resp = requests.get(
            url,
            headers={"User-Agent": "GNSM-AI-Docent/1.0"},
            timeout=10,
        )
        resp.raise_for_status()
        html = resp.text
        facts = _html_to_facts(html)
        tables_md, image_urls = _extract_tables_and_images_for_display(html)
        title = _extract_page_title(html)
        has_rich = bool(tables_md or image_urls)

        return {
            "source": url,
            "title": title,
            "facts": facts,
            "has_rich": has_rich,
        }

    except Exception:
        logging.error(f"[LIVE] í˜ì´ì§€ ë¡œë“œ ì‹¤íŒ¨:\n{traceback.format_exc()}")
        return {"source": url, "error": "í˜ì´ì§€ ë¡œë“œ ì‹¤íŒ¨"}


# ---------------------------------------
# 4. ê³¼í•™ê´€ ì£¼ìš” í˜ì´ì§€ ë§¤í•‘
# ---------------------------------------
LIVE_PAGES = {
    # --- í•µì‹¬ ìš´ì˜/ì˜ˆì•½ ---
    "í™ˆí˜ì´ì§€": f"{MUSEUM_BASE_URL}/",
    "ê³µì§€ì‚¬í•­": f"{MUSEUM_BASE_URL}/scipia/introduce/notice",
    "ì´ìš©ì•ˆë‚´": f"{MUSEUM_BASE_URL}/scipia/guide/totalGuide",
    "ì£¼ì°¨ì•ˆë‚´": f"{MUSEUM_BASE_URL}/scipia/introduce/parking",
    "ì—°ê°„íšŒì›": f"{MUSEUM_BASE_URL}/scipia/guide/paidMember",
    "ë‹¨ì²´ê´€ëŒ": f"{MUSEUM_BASE_URL}/scipia/guide/groupTours",
    "ì¶”ì²œê´€ëŒì½”ìŠ¤": f"{MUSEUM_BASE_URL}/scipia/guide/recommendCourse",
    "ê´€ëŒê°ëŒ€í”¼": f"{MUSEUM_BASE_URL}/scipia/communication/safety",
    "ì•ˆì „ì‚¬ê³ ìˆ˜ì¹™": f"{MUSEUM_BASE_URL}/scipia/communication/safetyRule",
    "í¸ì˜ì‹œì„¤": f"{MUSEUM_BASE_URL}/scipia/guide/convenience",
    "ì‹ìŒì‹œì„¤": f"{MUSEUM_BASE_URL}/scipia/guide/food",
    "êµí†µì•ˆë‚´": f"{MUSEUM_BASE_URL}/scipia/introduce/location",

    # --- í–‰ì‚¬ ë° ê³µì—° ---
    "í–‰ì‚¬": f"{MUSEUM_BASE_URL}/scipia/events/list/culture",
    "ê³µì—°": f"{MUSEUM_BASE_URL}/scipia/events/list/play",

    # --- ì²œë¬¸ìš°ì£¼ê´€ ---
    "ì²œì²´íˆ¬ì˜ê´€ ì†Œê°œ": f"{MUSEUM_BASE_URL}/scipia/display/planetarium",
    "ì²œì²´íˆ¬ì˜ê´€ ìš´ì˜": f"{MUSEUM_BASE_URL}/scipia/introduce/notice/24281",
    "ì²œì²´íˆ¬ì˜ê´€ í”„ë¡œê·¸ë¨": f"{MUSEUM_BASE_URL}/scipia/introduce/notice/24281",
    "ì²œì²´íˆ¬ì˜ê´€ ì˜ˆì•½": f"{MUSEUM_BASE_URL}/scipia/schedules?ACADEMY_CD=ACD007&CLASS_CD=CL7001",
    "ì²œì²´íˆ¬ì˜ê´€ ë‹¨ì²´": f"{MUSEUM_BASE_URL}/scipia/introduce/notice/23441",

    "ì²œë¬¸ëŒ€ ì†Œê°œ": f"{MUSEUM_BASE_URL}/scipia/display/planetarium/observation",
    "ì²œë¬¸ëŒ€ ìš´ì˜": f"{MUSEUM_BASE_URL}/scipia/introduce/notice/25098",
    "ì²œë¬¸ëŒ€ í”„ë¡œê·¸ë¨": f"{MUSEUM_BASE_URL}/scipia/introduce/notice/25098",
    "ì²œë¬¸ëŒ€ ì˜ˆì•½": f"{MUSEUM_BASE_URL}/scipia/schedules?ACADEMY_CD=ACD007&CLASS_CD=CL7003",
    "ì²œë¬¸ëŒ€ ë‹¨ì²´": f"{MUSEUM_BASE_URL}/scipia/introduce/notice/25100",

    "ìŠ¤í˜ì´ìŠ¤ ì•„ë‚ ë¡œê·¸ í”„ë¡œê·¸ë¨": f"{MUSEUM_BASE_URL}/scipia/display/planetarium/spaceAnalog",
    "ìŠ¤í˜ì´ìŠ¤ ì•„ë‚ ë¡œê·¸ ì˜ˆì•½": f"{MUSEUM_BASE_URL}/scipia/schedules?ACADEMY_CD=ACD007&CLASS_CD=CL7002",
    "ìŠ¤í˜ì´ìŠ¤ ì•„ë‚ ë¡œê·¸ ë‹¨ì²´": f"{MUSEUM_BASE_URL}/scipia/introduce/notice/24400",

    # --- ìƒì„¤ì „ì‹œê´€ ---
    "ìì—°ì‚¬ê´€": f"{MUSEUM_BASE_URL}/scipia/display/mainBuilding/naturalHistory",
    "ì²¨ë‹¨ê¸°ìˆ ê´€": f"{MUSEUM_BASE_URL}/scipia/display/mainBuilding/advancedTechnology2",
    "ê³¼í•™íƒêµ¬ê´€": f"{MUSEUM_BASE_URL}/scipia/display/mainBuilding/basicScience",
    "í•œêµ­ë¬¸ëª…ê´€": f"{MUSEUM_BASE_URL}/scipia/display/mainBuilding/traditionalSciences",
    "ë¯¸ë˜ìƒìƒSFê´€": f"{MUSEUM_BASE_URL}/scipia/display/mainBuilding/sfSpecial",
    "ìœ ì•„ì²´í—˜ê´€": f"{MUSEUM_BASE_URL}/scipia/display/mainBuilding/kidsPlayground",
    "ëª…ì˜ˆì˜ì „ë‹¹": f"{MUSEUM_BASE_URL}/scipia/display/frontier/hallOfFame",
    "íŠ¹ë³„ê¸°íšì „": f"{MUSEUM_BASE_URL}/scipia/events/list/exhibition#n",

    # --- ì „ì‹œì—°ê³„ ---
    "ì²´í—˜ì „ì‹œë¬¼ ì˜ˆì•½": f"{MUSEUM_BASE_URL}/scipia/display/displayExperience",
    "ì „ì‹œì¥ í”„ë¡œê·¸ë¨ ì•ˆë‚´": f"{MUSEUM_BASE_URL}/scipia/introduce/notice/25399",
    "ì „ì‹œí•´ì„¤": f"{MUSEUM_BASE_URL}/scipia/display/displayExplanation",

    # --- ì•¼ì™¸ì „ì‹œê´€ ---
    "ê³¤ì¶©ìƒíƒœê´€": f"{MUSEUM_BASE_URL}/scipia/display/outdoorEcological/insectarium",
    "ìƒíƒœê³µì›": f"{MUSEUM_BASE_URL}/scipia/display/outdoorEcological/ecoPark",
    "ê³µë£¡ê³µì›": f"{MUSEUM_BASE_URL}/scipia/display/outdoorEcological/dinosaurAndHistory",
    "ì˜¥ì™¸ì „ì‹œì¥": f"{MUSEUM_BASE_URL}/scipia/display/outdoorEcological/outdoor",

    # --- ì†Œí†µ ë° ì†Œê°œ ---
    "ì¸ì‚¬ë§": f"{MUSEUM_BASE_URL}/scipia/introduce/chief",
    "ì—°í˜": f"{MUSEUM_BASE_URL}/scipia/introduce/history",
    "ì¡°ì§ ë° ì—°í˜": f"{MUSEUM_BASE_URL}/scipia/introduce/organization",
    "ì£¼ë³€ì‹œì„¤": f"{MUSEUM_BASE_URL}/scipia/introduce/surround",
    "ìœ ê´€ê¸°ê´€": f"{MUSEUM_BASE_URL}/scipia/introduce/familySites",
    "ìˆ˜ë„ê¶Œê³¼í•™ê´€": f"{MUSEUM_BASE_URL}/scipia/introduce/capitalScience",
    "ë³´ë„ìë£Œ": f"{MUSEUM_BASE_URL}/scipia/introduce/report",
    "í˜„ì¥ìŠ¤ì¼€ì¹˜": f"{MUSEUM_BASE_URL}/scipia/introduce/sketch",
    "ì±„ìš©ê³µê³ ": f"{MUSEUM_BASE_URL}/scipia/introduce/recruit",
    "ì¼ë°˜ìë£Œì‹¤": f"{MUSEUM_BASE_URL}/scipia/communication/normalLibrary",
    "ê·œì •ìë£Œì‹¤": f"{MUSEUM_BASE_URL}/scipia/communication/roleLibrary",
    "ìì£¼ë¬»ëŠ”ì§ˆë¬¸": f"{MUSEUM_BASE_URL}/scipia/communication/faq/faqTotal",
    "ì˜ê²¬ìˆ˜ë ´": f"{MUSEUM_BASE_URL}/scipia/communication/opinions",
    "ê³¼í•™ìë£Œì‹¤": f"{MUSEUM_BASE_URL}/scipia/references",
    "ìì›ë´‰ì‚¬": f"{MUSEUM_BASE_URL}/scipia/schedules/voluntary",
}


# ---------------------------------------
# 5. ì£¼ì œ íŠ¸ë¦¬ (ëŒ€ë¶„ë¥˜ -> ì¤‘/ì†Œë¶„ë¥˜)
# ---------------------------------------
TOPIC_TREE = {
    "guide": {
        "label": "ê´€ëŒ ì´ìš©ì•ˆë‚´",
        "children": [
            ("ì´ìš©ì•ˆë‚´ ì „ì²´", "ì´ìš©ì•ˆë‚´(ê´€ëŒì‹œê°„, ìš”ê¸ˆ, íœ´ê´€ì¼) ì•Œë ¤ì¤˜"),
            ("ê´€ëŒìš”ê¸ˆ", "ê´€ëŒìš”ê¸ˆ ì•Œë ¤ì¤˜"),
            ("íœ´ê´€ì¼/ìš´ì˜ì¼", "íœ´ê´€ì¼/ìš´ì˜ì¼ ì•Œë ¤ì¤˜"),
            ("ì—°ê°„íšŒì›", "ì—°ê°„íšŒì› ì•ˆë‚´í•´ì¤˜"),
            ("ì£¼ì°¨ì•ˆë‚´", "ì£¼ì°¨ì•ˆë‚´ ì•Œë ¤ì¤˜"),
            ("êµí†µì•ˆë‚´", "êµí†µì•ˆë‚´ ì•Œë ¤ì¤˜"),
        ],
    },
    "astro_program": {
        "label": "ì²œë¬¸ìš°ì£¼ì‹œì„¤",
        "children": [
            ("ì²œì²´íˆ¬ì˜ê´€ í”„ë¡œê·¸ë¨", "ì²œì²´íˆ¬ì˜ê´€ í”„ë¡œê·¸ë¨ ì•Œë ¤ì¤˜"),
            ("ì²œì²´íˆ¬ì˜ê´€ ë‹¨ì²´", "ì²œì²´íˆ¬ì˜ê´€ ë‹¨ì²´ í”„ë¡œê·¸ë¨ ì•ˆë‚´í•´ì¤˜"),
            ("ì²œë¬¸ëŒ€ í”„ë¡œê·¸ë¨", "ì²œë¬¸ëŒ€ í”„ë¡œê·¸ë¨ ì•Œë ¤ì¤˜"),
            ("ì²œë¬¸ëŒ€ ë‹¨ì²´", "ì²œë¬¸ëŒ€ ë‹¨ì²´ í”„ë¡œê·¸ë¨ ì•ˆë‚´í•´ì¤˜"),
            ("ìŠ¤í˜ì´ìŠ¤ ì•„ë‚ ë¡œê·¸ í”„ë¡œê·¸ë¨", "ìŠ¤í˜ì´ìŠ¤ ì•„ë‚ ë¡œê·¸ í”„ë¡œê·¸ë¨ ì•Œë ¤ì¤˜"),
            ("ìŠ¤í˜ì´ìŠ¤ ì•„ë‚ ë¡œê·¸ ë‹¨ì²´", "ìŠ¤í˜ì´ìŠ¤ ì•„ë‚ ë¡œê·¸ í”„ë¡œê·¸ë¨ ì•Œë ¤ì¤˜"),
        ],
    },
    "exhibition": {
        "label": "ìƒì„¤ì „ì‹œê´€",
        "children": [
            ("ìì—°ì‚¬ê´€", "ìì—°ì‚¬ê´€ ì•ˆë‚´í•´ì¤˜"),
            ("ì²¨ë‹¨ê¸°ìˆ ê´€", "ì²¨ë‹¨ê¸°ìˆ ê´€ ì•ˆë‚´í•´ì¤˜"),
            ("ê³¼í•™íƒêµ¬ê´€", "ê³¼í•™íƒêµ¬ê´€ ì•ˆë‚´í•´ì¤˜"),
            ("í•œêµ­ë¬¸ëª…ê´€", "í•œêµ­ë¬¸ëª…ê´€ ì•ˆë‚´í•´ì¤˜"),
            ("ë¯¸ë˜ìƒìƒSFê´€", "ë¯¸ë˜ìƒìƒSFê´€ ì•ˆë‚´í•´ì¤˜"),
            ("ìœ ì•„ì²´í—˜ê´€", "ìœ ì•„ì²´í—˜ê´€ ì•ˆë‚´í•´ì¤˜"),
            ("ëª…ì˜ˆì˜ì „ë‹¹", "ëª…ì˜ˆì˜ ì „ë‹¹ ì•ˆë‚´í•´ì¤˜"),
            ("íŠ¹ë³„ê¸°íšì „", "íŠ¹ë³„ê¸°íšì „ ì•ˆë‚´í•´ì¤˜"),
        ],
    },
    "group_program": {
        "label": "ë‹¨ì²´Â·ì „ì‹œì—°ê³„ í”„ë¡œê·¸ë¨",
        "children": [
            ("ë‹¨ì²´ê´€ëŒ", "ë‹¨ì²´ê´€ëŒ ì•ˆë‚´í•´ì¤˜"),
            ("ì „ì‹œí•´ì„¤", "ì „ì‹œí•´ì„¤ í”„ë¡œê·¸ë¨ ì•ˆë‚´í•´ì¤˜"),
            ("ì²´í—˜ì „ì‹œë¬¼ ì˜ˆì•½", "ì²´í—˜ì „ì‹œë¬¼ ì˜ˆì•½ ì•ˆë‚´í•´ì¤˜"),
        ],
    },
    "outdoor": {
        "label": "ì•¼ì™¸ì „ì‹œÂ·ìƒíƒœ",
        "children": [
            ("ê³¤ì¶©ìƒíƒœê´€", "ê³¤ì¶©ìƒíƒœê´€ ì•ˆë‚´í•´ì¤˜"),
            ("ê³µë£¡ê³µì›", "ê³µë£¡ê³µì› ì•ˆë‚´í•´ì¤˜"),
            ("ìƒíƒœê³µì›", "ìƒíƒœê³µì› ì•ˆë‚´í•´ì¤˜"),
            ("ì˜¥ì™¸ì „ì‹œì¥", "ì˜¥ì™¸ì „ì‹œì¥ ì•ˆë‚´í•´ì¤˜"),
        ],
    },
    "facility": {
        "label": "í¸ì˜ì‹œì„¤Â·ì‹ìŒÂ·êµí†µ",
        "children": [
            ("ì‹ìŒì‹œì„¤", "ì‹ìŒì‹œì„¤(ì¹´í˜Â·ì‹ë‹¹) ì•ˆë‚´í•´ì¤˜"),
            ("í¸ì˜ì‹œì„¤", "í¸ì˜ì‹œì„¤ ì•ˆë‚´í•´ì¤˜"),
            ("ì£¼ì°¨ì•ˆë‚´", "ì£¼ì°¨ì•ˆë‚´ ì•Œë ¤ì¤˜"),
            ("êµí†µì•ˆë‚´", "êµí†µì•ˆë‚´ ì•Œë ¤ì¤˜"),
        ],
    },
    "etc": {
        "label": "FAQÂ·ê³µì§€Â·ê¸°íƒ€",
        "children": [
            ("ìì£¼ë¬»ëŠ”ì§ˆë¬¸", "ìì£¼ ë¬»ëŠ” ì§ˆë¬¸ ì•Œë ¤ì¤˜"),
            ("ê³µì§€ì‚¬í•­", "ê³µì§€ì‚¬í•­ ì•ˆë‚´í•´ì¤˜"),
            ("ê³¼í•™ìë£Œì‹¤", "ê³¼í•™ìë£Œì‹¤ ì•ˆë‚´í•´ì¤˜"),
        ],
    },
}


# ---------------------------------------
# 6. ì˜¤íƒ€/ìœ ì‚¬ë„ ê¸°ë°˜ LIVE_PAGES í‚¤ì›Œë“œ ì¶”ì •
# ---------------------------------------
def _guess_live_key(question: str, cutoff: float = 0.6):
    q = re.sub(r"\s+", "", question)  # ê³µë°± ì œê±°

    best_key = None
    best_score = 0.0

    for key in LIVE_PAGES.keys():
        key_norm = key.replace(" ", "")

        score = difflib.SequenceMatcher(None, key_norm, q).ratio()

        if key_norm in q or q in key_norm:
            score = max(score, 0.99)

        if score > best_score:
            best_score = score
            best_key = key

    if best_key and best_score >= cutoff:
        return best_key

    return None


def _match_live_keys(question: str):
    q = re.sub(r"\s+", "", question)
    matched = []

    for key in LIVE_PAGES.keys():
        if key.replace(" ", "") in q:
            matched.append(key)

    if not matched:
        guessed = _guess_live_key(question)
        if guessed:
            matched.append(guessed)
            st.info(f"í˜¹ì‹œ **'{guessed}'**(ì„)ë¥¼ ì˜ë¯¸í•˜ì‹  ê±´ê°€ìš”? í•´ë‹¹ í˜ì´ì§€ ê¸°ì¤€ìœ¼ë¡œ ì•ˆë‚´ë“œë¦´ê²Œìš”.")

    time_fee_keywords = [
        "ìš´ì˜ì‹œê°„",
        "ê´€ëŒì‹œê°„",
        "ê°œê´€ì‹œê°„",
        "íê´€ì‹œê°„",
        "ê´€ëŒìš”ê¸ˆ",
        "ê´€ëŒë£Œ",
        "ì…ì¥ë£Œ",
        "ìš”ê¸ˆ",
        "ê´€ëŒì¼",
        "ìš´ì˜ì¼",
        "ê°œê´€ì¼",
        "íœ´ê´€ì¼",
        "íœ´ë¬´ì¼",
        "íœ´ê´€",
        "íœ´ë¬´",
        "ìš´ì˜ì¼ì •",
        "ê°œê´€ì¼ì •",
    ]
    if any(kw in q for kw in time_fee_keywords):
        if "ì´ìš©ì•ˆë‚´" not in matched:
            matched.append("ì´ìš©ì•ˆë‚´")

    return matched


# ---------------------------------------
# 7. ì‚¬ì´íŠ¸ ì „ì²´ ê²€ìƒ‰ (indexer ì œê±° ë²„ì „: í˜„ì¬ëŠ” ì‚¬ìš© ì•ˆ í•¨)
# ---------------------------------------
def _search_site(query: str, limit: int = 5):
    """
    ì˜ˆì „ì—ëŠ” indexer ê¸°ë°˜ ì „ì²´ ê²€ìƒ‰ì„ ì‚¬ìš©í–ˆì§€ë§Œ,
    í˜„ì¬ ë°°í¬ ë²„ì „ì—ì„œëŠ” indexerë¥¼ ì œê±°í–ˆìœ¼ë¯€ë¡œ ë¹ˆ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    (í•„ìš”í•˜ë©´ ë‚˜ì¤‘ì— ë‹¤ë¥¸ ê²€ìƒ‰ ë°©ì‹ìœ¼ë¡œ êµì²´)
    """
    logging.info(f"[SEARCH] (indexer ì œê±°) ê²€ìƒ‰ í˜¸ì¶œ: {query} (limit={limit})")
    return []


# ---------------------------------------
# 8. LLM ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ & í‘¸í„°
# ---------------------------------------
STRICT_SYSTEM_PROMPT = """
ë‹¹ì‹ ì€ êµ­ë¦½ê³¼ì²œê³¼í•™ê´€ ì „ìš© AI ê°€ì´ë“œì…ë‹ˆë‹¤.

[ì—­í• ]
- ì‚¬ìš©ìê°€ ë¬»ëŠ” ë‚´ìš©ì„, ì•„ë˜ FACTSì— í¬í•¨ëœ ì •ë³´ë§Œ ì‚¬ìš©í•´ì„œ ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…í•©ë‹ˆë‹¤.
- FACTSëŠ” êµ­ë¦½ê³¼ì²œê³¼í•™ê´€ ê³µì‹ í™ˆí˜ì´ì§€ì—ì„œ ê°€ì ¸ì˜¨ ì‹¤ì œ ë‚´ìš©ì…ë‹ˆë‹¤.

[ì—„ê²©í•œ ê·œì¹™]
1. FACTS ë¸”ë¡ ì•ˆì˜ ë¬¸ì¥ì„ ê·¸ëŒ€ë¡œ ê¸¸ê²Œ ë³µì‚¬Â·ë¶™ì—¬ë„£ê¸° í•˜ì§€ ë§ˆì„¸ìš”.
2. FACTS ë¸”ë¡(ì˜ˆ: '### í…ìŠ¤íŠ¸', '### í‘œ', '[ì„¹ì…˜:' ë“±)ì˜ êµ¬ì¡°ë‚˜ ë¬¸êµ¬ë¥¼
   ë‹µë³€ì— ê·¸ëŒ€ë¡œ ë³´ì—¬ì£¼ì§€ ë§ˆì„¸ìš”.
3. FACTSì—ì„œ í•„ìš”í•œ ì •ë³´ë§Œ ë½‘ì•„ì„œ **ì§§ì€ ë¶ˆë¦¿/í‘œ í˜•íƒœë¡œ ì •ë¦¬ë§Œ** í•´ì£¼ì„¸ìš”.
4. ë‹µë³€ì€ ìµœëŒ€ 15ì¤„ ì´ë‚´ë¡œ, ê° ì¤„ì€ í•œë‘ ë¬¸ì¥ìœ¼ë¡œë§Œ ì‘ì„±í•˜ì„¸ìš”.
5. FACTSì— ì—†ëŠ” ì •ë³´(ìˆ«ì, ë‚ ì§œ, ìš”ê¸ˆ, ì‹œê°„, í”„ë¡œê·¸ë¨ëª… ë“±)ëŠ” ì ˆëŒ€ë¡œ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”.
6. FACTSì— ì—†ëŠ” ë¶€ë¶„ì´ ìˆë”ë¼ë„ í•´ë‹¹ ë¶€ë¶„ì€ ìƒëµí•˜ê³ ,
   FACTSì—ì„œ í™•ì¸ ê°€ëŠ¥í•œ ì •ë³´ë§Œ ì •ë¦¬í•´ì„œ ë³´ì—¬ì£¼ì„¸ìš”.
7. ì‚¬ìš©ìì—ê²Œ "FACTS ì—†ìŒ", "í™ˆí˜ì´ì§€ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤" ê°™ì€
   ì˜¤ë¥˜ ì•ˆë‚´ ë¬¸êµ¬ë¥¼ ì¶œë ¥í•˜ì§€ ë§ˆì„¸ìš”. (ê·¸ ë¬¸êµ¬ëŠ” ì½”ë“œì—ì„œë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.)

[ì¶œë ¥ í˜•ì‹]
- í•­ìƒ ë§ˆí¬ë‹¤ìš´(Markdown) í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.
- ì²« ì¤„ì—ëŠ” ê°„ë‹¨í•œ ì œëª©ì„ `### ì œëª©` í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.
- ê·¸ ì•„ë˜ëŠ” ì¤„ê¸€ì´ ì•„ë‹ˆë¼ í•µì‹¬ í•­ëª©ì„ ë¶ˆë¦¿(`- í•­ëª©`)ì´ë‚˜ ê°„ë‹¨í•œ í‘œë¡œ ì •ë¦¬í•˜ì„¸ìš”.
- ìš´ì˜ì‹œê°„/ìš”ê¸ˆ/ëŒ€ìƒ/ì°¸ê°€ì¸ì›/ì˜ˆì•½ë°©ë²•/í”„ë¡œê·¸ë¨ ë‚´ìš©ì„ ê°ê° í•­ëª©ë³„ë¡œ ë¶„ë¦¬í•´ì„œ ì¨ ì£¼ì„¸ìš”.
"""


def _append_info_footer(answer: str) -> str:
    """
    ì´ì „ ë²„ì „ì˜ 'ì•ˆë‚´ë“œë¦½ë‹ˆë‹¤!' ê³µí†µ ë¬¸êµ¬ëŠ” ì œê±°.
    ì§€ê¸ˆì€ ì•„ë¬´ ê²ƒë„ ì¶”ê°€í•˜ì§€ ì•Šê³  ê·¸ëŒ€ë¡œ ë°˜í™˜.
    """
    return answer


# ---------------------------------------
# 9. LLM ì´ˆê¸°í™”
# ---------------------------------------
def _init_llm(model_name: str = "gpt-4o-mini", temperature: float = 0.0) -> ChatOpenAI:
    api_key = st.secrets.get("OPENAI_API_KEY", "") or os.getenv("OPENAI_API_KEY", "")
    if not api_key:
        raise RuntimeError("OPENAI API Keyê°€ ì—†ìŠµë‹ˆë‹¤.")
    return ChatOpenAI(
        temperature=temperature,
        model=model_name,
        openai_api_key=api_key,
    )


# ---------------------------------------
# 10. ì£¼ì œ ë²„íŠ¼ + ëŒì•„ê°€ê¸°
# ---------------------------------------
def _render_topic_shortcuts():
    stage = st.session_state.get("topic_stage", "root")
    group = st.session_state.get("topic_group", None)

    st.markdown("#### ğŸ” ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?")

    if stage == "root" or group not in TOPIC_TREE:
        cols = st.columns(3)
        for i, (group_key, meta) in enumerate(TOPIC_TREE.items()):
            label = meta["label"]
            col = cols[i % 3]
            with col:
                if st.button(label, key=f"topic_root_{group_key}"):
                    st.session_state["topic_stage"] = "mid"
                    st.session_state["topic_group"] = group_key
                    st.rerun()
        return

    meta = TOPIC_TREE[group]
    st.markdown(f"##### ğŸ“Œ '{meta['label']}'ì—ì„œ ë” ê¶ê¸ˆí•œ ë‚´ìš©ì„ ê³¨ë¼ë³´ì„¸ìš”")

    children = meta.get("children", [])
    cols = st.columns(3)
    for i, (label, query) in enumerate(children):
        col = cols[i % 3]
        with col:
            if st.button(label, key=f"topic_child_{group}_{i}"):
                st.session_state["pending_query"] = query

    back_col, _ = st.columns([1, 3])
    with back_col:
        if st.button("â¬… ëŒì•„ê°€ê¸°", key="topic_back_root", type="primary"):
            st.session_state["topic_stage"] = "root"
            st.session_state["topic_group"] = None
            st.session_state["pending_query"] = ""
            st.rerun()


def _render_global_back_button():
    if st.button("â¬… ëŒì•„ê°€ê¸°", key="global_back_to_topics", type="primary"):
        st.session_state["topic_stage"] = "root"
        st.session_state["topic_group"] = None
        st.session_state["pending_query"] = ""
        st.rerun()


# ---------------------------------------
# 11. Streamlit ë©”ì¸ í•¨ìˆ˜
# ---------------------------------------
def run_chat_assistant(
    model_name: str = "gpt-4o-mini",
    temperature: float = 0.0,
    system_prompt: str = STRICT_SYSTEM_PROMPT,
) -> None:
    """
    Grounded LLM êµ¬ì¡°:
      1) í™ˆí˜ì´ì§€ì—ì„œ FACTS(ì „ì²´ ë‚´ìš©)ë¥¼ ëª¨ì€ë‹¤.
      2) FACTSë§Œ LLMì— ë„˜ê²¨ì„œ ë§íˆ¬/ìš”ì•½ë§Œ ìˆ˜í–‰ (ì¶”ê°€ ì •ë³´ ê¸ˆì§€)
      3) ì¶œì²˜ í˜ì´ì§€ì— í‘œ/ì´ë¯¸ì§€ê°€ ìˆìœ¼ë©´
         â†’ ë‹µë³€ ë’¤, 'ë” ìì„¸íˆ ë³´ê¸°' ë²„íŠ¼ ì•ì—
           'í™ˆí˜ì´ì§€ ì•ˆë‚´ì‚¬í•­ì„ ê¼­ í•¨ê»˜ ë³´ë¼'ëŠ” ì•ˆë‚´ ë¬¸êµ¬ë¥¼ ë³´ì—¬ì¤€ë‹¤.
    """

    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    if "topic_stage" not in st.session_state:
        st.session_state["topic_stage"] = "root"
    if "topic_group" not in st.session_state:
        st.session_state["topic_group"] = None
    if "pending_query" not in st.session_state:
        st.session_state["pending_query"] = ""
    if "messages" not in st.session_state:
        # ğŸ‘‰ ì¸ì‚¬ë§ì€ messagesì— ë„£ì§€ ì•Šê³ , í™”ë©´ì—ë§Œ í•œ ë²ˆ ê·¸ë¦°ë‹¤.
        st.session_state.messages = []

    # LLM ì´ˆê¸°í™”
    try:
        llm = _init_llm(model_name=model_name, temperature=temperature)
    except Exception as e:  # pragma: no cover
        st.error(f"âš ï¸ LLM ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return

    # ì¸ì‚¬ë§: ëŒ€í™” ë‚´ì—­ì´ í•˜ë‚˜ë„ ì—†ì„ ë•Œë§Œ í•œ ë²ˆ ë³´ì—¬ì£¼ê¸°
    if not st.session_state.messages:
        with st.chat_message("assistant"):
            st.markdown("#### ì•ˆë…•í•˜ì„¸ìš”! êµ­ë¦½ê³¼ì²œê³¼í•™ê´€ AI ê°€ì´ë“œì…ë‹ˆë‹¤ ğŸ¤–\n\n")

    # ê¸°ì¡´ ëŒ€í™” ì¶œë ¥
    for msg in st.session_state.messages:
        role = "assistant" if isinstance(msg, AIMessage) else "user"
        with st.chat_message(role):
            if role == "assistant":
                st.markdown(msg.content, unsafe_allow_html=True)
            else:
                st.markdown(msg.content)

    st.markdown("---")
    _render_topic_shortcuts()

    pending = st.session_state.get("pending_query", "")
    user_input = st.chat_input("ê¶ê¸ˆí•œ ë‚´ìš©ì„ ì…ë ¥í•´ ì£¼ì„¸ìš” (ì˜ˆ: ì²œë¬¸ëŒ€ ë‹¨ì²´ í”„ë¡œê·¸ë¨ ì•Œë ¤ì¤˜)")

    if pending:
        user_msg = pending
        st.session_state["pending_query"] = ""
    elif user_input:
        user_msg = user_input
    else:
        return

    user_msg_obj = HumanMessage(content=user_msg)
    st.session_state.messages.append(user_msg_obj)
    with st.chat_message("user"):
        st.markdown(user_msg)

    # 1ë‹¨ê³„: FACTS + "í‘œ/ì´ë¯¸ì§€ ì¡´ì¬ ì—¬ë¶€" ìˆ˜ì§‘
    facts_sections = []
    link_items = []
    has_rich_content = False  # ì–´ë–¤ ì¶œì²˜ë“  í‘œë‚˜ ì´ë¯¸ì§€ê°€ ìˆìœ¼ë©´ True

    # (1) LIVE_PAGES
    matched_keys = _match_live_keys(user_msg)
    for key in matched_keys:
        url = LIVE_PAGES[key]
        page_data = _fetch_page(url)
        src = page_data.get("source", url)
        title = page_data.get("title") or key

        link_items.append((title, src))

        if page_data.get("has_rich"):
            has_rich_content = True

        if "facts" in page_data:
            section = f"[ì„¹ì…˜: {title}]\n{page_data['facts']}"
        else:
            section = (
                f"[ì„¹ì…˜: {title}]\n"
                "ì´ í˜ì´ì§€ëŠ” ìë™ìœ¼ë¡œ ë‚´ìš©ì„ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. "
                f"ì•„ë˜ ë§í¬ë¥¼ ëˆŒëŸ¬ í™ˆí˜ì´ì§€ì—ì„œ ì§ì ‘ í™•ì¸í•´ ì£¼ì„¸ìš”. ({src})"
            )
        facts_sections.append(section)

    # (2) ì‚¬ì´íŠ¸ ì „ì²´ ê²€ìƒ‰ (í˜„ì¬ indexer ì œê±°ë¡œ ì¸í•´ í•­ìƒ ë¹ˆ ê²°ê³¼)
    hits = _search_site(user_msg, limit=3)
    for h in hits:
        url = h.get("url") or ""
        raw_title = h.get("title") or "ê´€ë ¨ í˜ì´ì§€"
        snippet = h.get("snippet") or ""

        page_data = None
        if url:
            page_data = _fetch_page(url)
            src = page_data.get("source", url)
        else:
            src = url

        display_title = raw_title
        if page_data is not None:
            display_title = page_data.get("title") or raw_title

        if url and all(u != url for _, u in link_items):
            link_items.append((display_title, url))

        if page_data is not None and page_data.get("has_rich"):
            has_rich_content = True

        if url and page_data is not None and "facts" in page_data:
            section = f"[ì„¹ì…˜: {display_title}]\n{page_data['facts']}"
        else:
            body = snippet or "í™ˆí˜ì´ì§€ì— ê´€ë ¨ í˜ì´ì§€ê°€ ìˆìŠµë‹ˆë‹¤. ë§í¬ë¥¼ ëˆŒëŸ¬ ë‚´ìš©ì„ í™•ì¸í•´ ì£¼ì„¸ìš”."
            section = f"[ì„¹ì…˜: {display_title}]\n{body}"
        facts_sections.append(section)

    facts_text = "\n\n----------------\n\n".join(facts_sections).strip()

    if not facts_text:
        body_lines = [
            "### í™ˆí˜ì´ì§€ì—ì„œ ì§ì ‘ í™•ì¸ì´ í•„ìš”í•œ ë‚´ìš©ì…ë‹ˆë‹¤.",
            "",
            "- ì§ˆë¬¸ê³¼ ì •í™•íˆ ì—°ê²°ë˜ëŠ” ì •ë³´ë¥¼ ìë™ìœ¼ë¡œ ì°¾ê¸° ì–´ë µìŠµë‹ˆë‹¤.",
            "- ì§ˆë¬¸ì„ ì¡°ê¸ˆ ë” êµ¬ì²´ì ìœ¼ë¡œ, ë˜ëŠ” ë‹¤ë¥¸ í‘œí˜„ìœ¼ë¡œ ë‹¤ì‹œ ì…ë ¥í•´ ì£¼ì„¸ìš” ğŸ˜Š",
            "",
            f"- [ì´ìš©ì•ˆë‚´ ë©”ì¸]({LIVE_PAGES['ì´ìš©ì•ˆë‚´']})",
            f"- [ì „ì²´ ë©”ë‰´ í•œëˆˆì— ë³´ê¸°]({MUSEUM_BASE_URL}/scipia/introduce/siteMap)",
        ]
        body = "\n".join(body_lines)
        answer = _append_info_footer(body)
        with st.chat_message("assistant"):
            st.markdown(answer, unsafe_allow_html=True)
            _render_global_back_button()
        st.session_state.messages.append(AIMessage(content=answer))
        return

    # 2ë‹¨ê³„: LLM í˜¸ì¶œ
    user_prompt = (
        "ì‚¬ìš©ì ì§ˆë¬¸:\n"
        f"{user_msg}\n\n"
        "ì•„ë˜ëŠ” êµ­ë¦½ê³¼ì²œê³¼í•™ê´€ í™ˆí˜ì´ì§€ì—ì„œ ê°€ì ¸ì˜¨ FACTS(ì›ë¬¸ ë°ì´í„°)ì…ë‹ˆë‹¤.\n"
        "- ì´ FACTSëŠ” ë‚´ë¶€ ì°¸ê³ ìš©ì´ë©°, ì‚¬ìš©ìì—ê²Œ ê·¸ëŒ€ë¡œ ë³´ì—¬ì£¼ë©´ ì•ˆ ë©ë‹ˆë‹¤.\n"
        "- FACTS ë¸”ë¡ ì† ë¬¸ì¥ê³¼ ì œëª©(ì˜ˆ: '### í…ìŠ¤íŠ¸', '### í‘œ', '[ì„¹ì…˜:' ë“±)ì„ ë³µì‚¬í•˜ì§€ ë§ê³ ,\n"
        "  í•„ìš”í•œ ì •ë³´ë§Œ ë½‘ì•„ì„œ ë¶ˆë¦¿/í‘œë¡œ ì§§ê²Œ ì •ë¦¬í•´ì„œ ë³´ì—¬ ì£¼ì„¸ìš”.\n\n"
        "FACTS ì‹œì‘\n"
        "----------------\n"
        f"{facts_text}\n"
        "----------------\n"
        "FACTS ë\n"
    )

    messages_for_llm = [
        SystemMessage(content=STRICT_SYSTEM_PROMPT),
        SystemMessage(
            content=(
                "ì¶”ê°€ ì§€ì‹œ(STRICT ê·œì¹™ê³¼ ëª¨ìˆœë˜ì§€ ì•ŠëŠ” ë²”ìœ„ì—ì„œë§Œ ë”°ë¥´ì„¸ìš”):\n"
                f"{system_prompt}"
            )
        ),
        HumanMessage(content=user_prompt),
    ]

    with st.chat_message("assistant"):
        with st.spinner("ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”!"):
            try:
                answer_obj = llm.invoke(messages_for_llm)
                answer_text = answer_obj.content
            except Exception as e:  # pragma: no cover
                answer_text = f"âš ï¸ ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"

            # ğŸ‘‰ ì·¨ì†Œì„ /ì‹œê°„í‘œê¸° ì •ë¦¬
            answer_text = _cleanup_answer_markdown(answer_text)

            # 3ë‹¨ê³„: "í™ˆí˜ì´ì§€ ê¼­ ë³´ê¸°" ì•ˆë‚´ ë¬¸êµ¬ (í‘œ/ì´ë¯¸ì§€ ìˆì„ ë•Œë§Œ)
            rich_notice_md = ""
            if has_rich_content:
                rich_notice_md = (
                    "\n\n> â„¹ï¸ **ë”ìš± ìì„¸í•œ ì•ˆë‚´ë¥¼ ì›í•˜ì‹ ë‹¤ë©´?**  \n"
                    "> í™ˆí˜ì´ì§€ë¥¼ í•¨ê»˜ í™•ì¸í•´ì£¼ì„¸ìš”!\n"
                )

            # 4ë‹¨ê³„: 'í™ˆí˜ì´ì§€ í™•ì¸í•˜ê¸°' ë²„íŠ¼
            links_md = ""
            if link_items:
                first_label, first_url = link_items[0]
                links_md = f"""
<a href="{first_url}" target="_blank" style="text-decoration:none;">
    <button style="padding:8px 16px; font-size:16px; border-radius:6px; border:1px solid #00519A; background-color:#00519A; color:white; cursor:pointer;">
        ğŸ” í™ˆí˜ì´ì§€ í™•ì¸í•˜ê¸°
    </button>
</a>
"""

            final_answer = answer_text + rich_notice_md + "\n\n" + links_md
            final_answer = _append_info_footer(final_answer)

            st.markdown(final_answer, unsafe_allow_html=True)
            _render_global_back_button()
            st.session_state.messages.append(AIMessage(content=final_answer))
